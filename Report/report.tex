\documentclass[11pt,twocolumn]{article}
% Packages
\usepackage{times}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{anysize}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{float}
\usepackage{natbib}
\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage[autolanguage]{numprint}
%\restylefloat{table}
\usepackage[tableposition=top]{caption}
% Macros
\newcommand{\np}{\numprint}
\newcommand{\tdl}{TD-$\lambda$ }
\newcommand{\cen}[1]{\begin{center}#1\end{center}}
% Options
\marginsize{1cm}{1.5cm}{1cm}{1.5cm}
% Title spec
\title{COMP3130 -- Group Project in Computer Science\\ 10$\times$10 Othello Learning Agent}
\date{June 8, 2012}
\author{Andrew Haigh -- u4667010;\\ Timothy Cosgrove -- u4843619;\\ Joshua Nelson -- u4850020}
% Section styling
\titleformat{\section}%
  {\large\itshape}%
  {\thesection.}{.5em}{}%
  [\vspace{1ex}\titlerule]%
\titleformat{\subsection}%
  {\itshape}%
  {\thesubsection.}{.5em}{}%
\begin{document}
\onecolumn
\maketitle
\clearpage
\section{Abstract}
An agent to play the board game \emph{Othello} is created, with the ability to learn through reinforcement. The minimax algorithm is used for game playing, and a static evaluation function for the leaf nodes is learnt by self play. The agent learns the insignificance of the number of stones, and the significance of stone positioning. This agent is played against itself, and against other developed agents, and it's performance is analysed.

\section{Problem overview}
%%Describe the game and the nature of the task
\section{Solution overview}
%%A minimax agent with a static evaluation function.
\section{Optimisations}
%% Alpha beta pruning, negamax, etc.
\section{Static evaluation function}
%% Necessity of a static evaluation function, features used, etc
\clearpage
\section{Learning}
\subsection{\tdl}
The \tdl algorithm was used to learn the weighting of the static evaluation function's various features. The implementation of this algorithm is based on the one implemented by the Knightcap agent (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.2003) %%TODO Reference this properly\\
At the end of each game played, the agent adjusts it's weights according to the following formula
\begin{center}

    $\displaystyle w := w + \alpha \sum _{t=1} ^{N-1} \Delta \tilde{J}(x_t,w) \: \times \: \Big[ \sum ^ {N-1} _{j=t} \lambda^{j-t} d_t \Big] $\\
        
    \begin{tabular}{  l l | l l }
      $w$                   & The vector of weights                         &$x_t$  & The $t^{th}$ board in the game \\
      $\alpha$              & The learning rate                             &$\lambda$      &The discount factor \\
      $N$                   & The number of states in the game              &$d_t$          &$\tilde{J}(x_{t+1},w) - \tilde{J}(x_t,w)$\\
      $\Delta \tilde{J}$    & The derivative of the $\tilde{J}$ function    &               &\\
    \end{tabular}
\end{center}

In the above formula, the $\tilde{J}$ function estimates the probability of winning from a given state, given a set of weights for features and our board. It approximates the $J$ function, \\
\begin{center}
$
J(x_t) = \begin{cases} 1, & \mbox{if } x_t\mbox{ is a winning state} \\ 0, & \mbox{if } x_t\mbox{ is a lost state} \end{cases}
$
\end{center}

For each game state $x_t$, we adjust the weights according to a factor $d_t$, which is the temporal difference. $d_t = \tilde{J}(x_{t+1},w) - \tilde{J}(x_t,w)$, and the weight adjustment is scaled with this amount. The key observation is, for the true $J$ function, $J(x_{t+1}) - J(x_t) = 0$, so we adjust our weights relative to this amount.

From Figure \ref{JFunctionGraphs_1000iterations}, we can see that the $\tilde{J}$ function, while not accurate for the entirety of the game, is able to predict the results within the last 20 moves. In comparison, Figure \ref{JFunctionGraphs_1iteration} indicates that the $\tilde{J}$ function initially is a bad approximation, as even after losing a game, the estimated probability of winning is high.
\begin{figure}[H]\centering

    \subfigure[Losing a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1000iteration_lost.pdf}}
    \subfigure[Winning a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1000iteration_won.pdf}}
    \caption{$\tilde{J}$ Function after 1000 learning iterations}
    \label{JFunctionGraphs_1000iterations}
\end{figure}
\begin{figure}[H]\centering

    \subfigure[Losing a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1iteration_lost.pdf}}
    \subfigure[Winning a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1iteration_won.pdf}}
    \caption{$\tilde{J}$ Function initially}
    \label{JFunctionGraphs_1iteration}
\end{figure}



In Figure \ref{LearningProgress}, we can see the progress of the learning agent as it plays against a fixed opponent. After about 1000 games, the agent settles on a win/loss ratio of approximately $0.7$.
The weights learnt were split into $20$ stages. Figure \ref{WeightsOverTime} shows the how highly the various features are weighted as a game progresses. We see that the `Legal Moves' feature is weighted highly early in the game, and later in the game, valued less. On the other hand, corner pieces and side pieces become more important as we approach the mid and end game. This makes sense, as the number of legal moves on the board is limited later in the game, and it is unlikely that a side or corner piece will be taken in the first quarter of the game.

\begin{figure}[H] %%Currently the formatting of these graphs are terrible. I have no idea how it can be fixed though unfortunately. Is that what you wanted?
\centering
    \subfigure{\includegraphics[trim= 6cm 2cm 5.5cm 3cm, clip, width=1\textwidth]{../Graphs/Learning_2ply_First1000.pdf}}
    \caption{Learning Progress of a 4-ply Learning player vs. a 4-ply Minimax player}
    \label{LearningProgress}
\end{figure}

\begin{figure}[H]\centering
    \subfigure{\includegraphics[trim= 0cm 0cm 1.5cm 1cm, clip, width=0.75\textwidth]{../Graphs/WeightsOverTime.pdf}} %%If you can come up with a better graph than this feel free to put it here
    \caption{Learnt weights for a 20-stage player, after 1000 learning iterations} 
    \label{WeightsOverTime20}
\end{figure}

\begin{figure}[H]\centering
    \subfigure{\includegraphics[trim= 0cm 0cm 1.5cm 1cm, clip, width=0.75\textwidth]{../Graphs/finalweights.pdf}}
    \caption{Learnt weights for a 92-stage player, after 30000 learning iterations} 
    \label{WeightsOverTime92}
\end{figure}

\begin{figure}[H]\centering
    \subfigure{\includegraphics[trim= 0cm 0cm 1.5cm 1cm, clip, width=0.75\textwidth]{../Graphs/legalmovesprogression.pdf}}
    \caption{The LegalMoves feature weights converging over time} 
    \label{LegalMovesConvergence}
\end{figure}
\clearpage
\subsection{ELO arena}
\subsection{Comparison of \tdl and ELO arena}
%% TD lambda and ELO arena
\section{Performance evaluation}
%% Tournament results, self play learning, fixed opponent results.
\section{Improvements}
%% Look at the performance and say what we could have improved.


\bibliographystyle{alpha}
\bibliography{ai}
\end{document}
