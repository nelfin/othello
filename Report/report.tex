\documentclass[11pt,twocolumn]{article}
% Packages
\usepackage{times}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{anysize}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{float}
\usepackage{natbib}
\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage[autolanguage]{numprint}
%\restylefloat{table}
\usepackage[tableposition=top]{caption}
% Macros
\newcommand{\np}{\numprint}
\newcommand{\tdl}{TD-$\lambda$ }
% Options
\marginsize{2.5cm}{1.5cm}{1cm}{1cm}
% Title spec
\title{COMP3130 -- Group Project in Computer Science\\ 10$\times$10 Othello Learning Agent}
\date{June 8, 2012}
\author{Andrew Haigh -- u4667010;\\ Timothy Cosgrove -- u4843619;\\ Joshua Nelson -- u4850020}
% Section styling
\titleformat{\section}%
  {\large\itshape}%
  {\thesection.}{.5em}{}%
  [\vspace{1ex}\titlerule]%
\titleformat{\subsection}%
  {\itshape}%
  {\thesubsection.}{.5em}{}%
\begin{document}
\onecolumn
\maketitle
\clearpage
\section{Abstract}
An agent to play the board game \emph{Othello} is created, with the ability to learn through reinforcement. The minimax algorithm is used for game playing, and a static evaluation function for the leaf nodes is learnt by self play. The agent learns the insignificance of the number of stones, and the significance of stone positioning. This agent is played against itself, and against other developed agents, and it's performance is analysed.

\section{Problem overview}
%%Describe the game and the nature of the task
\section{Solution overview}
%%A minimax agent with a static evaluation function.
\section{Optimisations}
%% Alpha beta pruning, negamax, etc.
\section{Static evaluation function}
%% Necessity of a static evaluation function, features used, etc
\clearpage
\section{Learning}
\subsection{\tdl}
The \tdl algorithm was used to learn the weighting of the static evaluation function's various features. The implementation of this algorithm is based on the one implemented by the Knightcap agent (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.2003) %%TODO Reference this properly\\
At the end of each game played, the agent adjusts it's weights according to the following formula
\begin{center}

    $\displaystyle w := w + \alpha \sum _{t=1} ^{N-1} \Delta \tilde{J}(x_t,w) \: \times \: \Big[ \sum ^ {N-1} _{j=t} \lambda^{j-t} d_t \Big] $\\
        
    \begin{tabular}{  l l | l l }
      $w$                   & The vector of weights                         &$x_t$  & The $t^{th}$ board in the game \\
      $\alpha$              & The learning rate                             &$\lambda$      &The discount factor \\
      $N$                   & The number of states in the game              &$d_t$          &$\tilde{J}(x_{t+1},w) - \tilde{J}(x_t,w)$\\
      $\Delta \tilde{J}$    & The derivative of the $\tilde{J}$ function    &               &\\
    \end{tabular}
\end{center}

In the above formula, the $\tilde{J}$ function estimates the probability of winning from a given state, given a set of weights for features and our board. It approximates the $J$ function, \\
\begin{center}
$
J(x_t) = \begin{cases} 1, & \mbox{if } x_t\mbox{ is a winning state} \\ 0, & \mbox{if } x_t\mbox{ is a lost state} \end{cases}
$
\end{center}

For each game state $x_t$, we adjust the weights according to a factor $d_t$, which is the temporal difference. $d_t = \tilde{J}(x_{t+1},w) - \tilde{J}(x_t,w)$, and the weight adjustment is scaled with this amount. The key observation is, for the true $J$ function, $J(x_{t+1}) - J(x_t) = 0$, so we adjust our weights relative to this amount.

From Figure \ref{JFunctionGraphs_1000iterations}, we can see that the $\tilde{J}$ function, while not accurate for the entirety of the game, is able to predict the results within the last 20 moves. In comparison, Figure \ref{JFunctionGraphs_1iteration} indicates that the $\tilde{J}$ function initially is a bad approximation, as even after losing a game, the estimated probability of winning is high.
\begin{figure}[H]

    \subfigure[Losing a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1000iteration_lost.pdf}}
    \subfigure[Winning a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1000iteration_won.pdf}}
    \caption{$\tilde{J}$ Function after 1000 learning iterations}
    \label{JFunctionGraphs_1000iterations}
\end{figure}
\begin{figure}[H]

    \subfigure[Losing a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1iteration_lost.pdf}}
    \subfigure[Winning a game]{\includegraphics[width=0.47\textwidth]{../Graphs/J_improved_1iteration_won.pdf}}
    \caption{$\tilde{J}$ Function initially}
    \label{JFunctionGraphs_1iteration}
\end{figure}
\subsection{ELO arena}
\subsection{Comparison of \tdl and ELO arena}
%% TD lambda and ELO arena
\section{Performance evaluation}
%% Tournament results, self play learning, fixed opponent results.
\section{Improvements}
%% Look at the performance and say what we could have improved.


\bibliographystyle{alpha}
\bibliography{ai}
\end{document}
