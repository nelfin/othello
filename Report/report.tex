\documentclass[11pt,twocolumn]{article}
% Packages
\usepackage{times}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{anysize}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{float}
\usepackage{natbib}
\usepackage[english]{babel}
\usepackage[autolanguage]{numprint}
%\restylefloat{table}
\usepackage[tableposition=top]{caption}
% Macros
\newcommand{\np}{\numprint}
\newcommand{\tdl}{TD-$\lambda$ }
% Options
\marginsize{2.5cm}{1.5cm}{1cm}{1cm}
% Title spec
\title{COMP3130 -- Group Project in Computer Science\\ 10$\times$10 Othello Learning Agent}
\date{June 8, 2012}
\author{Andrew Haigh -- u4667010;\\ Timothy Cosgrove -- u4843619;\\ Joshua Nelson -- u4850020}
% Section styling
\titleformat{\section}%
  {\large\itshape}%
  {\thesection.}{.5em}{}%
  [\vspace{1ex}\titlerule]%
\titleformat{\subsection}%
  {\itshape}%
  {\thesubsection.}{.5em}{}%
\begin{document}
\onecolumn
\maketitle
\clearpage
\section{Abstract}
An agent to play the board game \emph{Othello} is created, with the ability to learn through reinforcement. The minimax algorithm is used for game playing, and a static evaluation function for the leaf nodes is learnt by self play. The agent learns the insignificance of the number of stones, and the significance of stone positioning. This agent is played against itself, and against other developed agents, and it's performance is analysed.

\section{Problem overview}
%%Describe the game and the nature of the task
\section{Solution overview}
%%A minimax agent with a static evaluation function.
\section{Optimisations}
%% Alpha beta pruning, negamax, etc.
\section{Static evaluation function}
%% Necessity of a static evaluation function, features used, etc
\clearpage
\section{Learning}
\subsection{\tdl}
The \tdl algorithm was used to learn the weighting of the static evaluation function's various features. The implementation of this algorithm is based on the one implemented by the Knightcap agent (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.2003) %%TODO Reference this properly\\
At the end of each game played, the agent adjusts it's weights according to the following formula
\begin{center}
    $\displaystyle w := w + \alpha \sum _{t=1} ^{N-1} \Delta \tilde{J}(x_t,w) \: \times \: \Big[ \sum ^ {N-1} _{j=t} \lambda^{j-t} d_t \Big] $\\
    \begin{tabular}{  l l | l l }
      $w$                   & The vector of weights                         &$x_t$  & The $t^{th}$ board in the game \\
      $\alpha$              & The learning rate                             &$\lambda$      &The discount factor \\
      $N$                   & The number of states in the game              &$d_t$          &$\tilde{J}(x_{t+1},w) - \tilde{J}(x_t,w)$\\
      $\Delta \tilde{J}$    & The derivative of the $\tilde{J}$ function    &               &\\
    \end{tabular}
\end{center}
In the above formula, the $\tilde{J}$ function estimates the probability of winning from a given state, given a set of weights for features and our board. 
\subsection{ELO arena}
\subsection{Comparison of \tdl and ELO arena}
%% TD lambda and ELO arena
\section{Performance evaluation}
%% Tournament results, self play learning, fixed opponent results.
\section{Improvements}
%% Look at the performance and say what we could have improved.


\bibliographystyle{alpha}
\bibliography{ai}
\end{document}
